export default [
  '"use strict";',
  '',
  '// Do NOT modify the line below.',
  'import * as toxicity from "https://cdn.skypack.dev/@tensorflow-models/toxicity";',
  '// Do NOT modify the line above.',
  '',
  '/**',
  ' * Skip to line 117 to see the model being used.',
  '*/',
  '',
  'const threshold = 0.9;',
  '',
  'const toxicityModelFormElement = document.getElementById("toxicityModelForm");',
  'const analysisElement = document.getElementById("analysis");',
  'const analysisSummaryElement = document.getElementById("analysisSummary");',
  'const analysisSummaryTextElement = document.getElementById("analysisSummaryText");',
  'const analysisSummaryDetailsElement = document.getElementById("analysisSummaryDetails");',
  'const submitButtonElement = document.getElementById("submit");',
  '',
  'const renderAnalysisDetails = (offensiveParts) => {',
  '  const ul = document.createElement("ul");',
  '  offensiveParts.forEach((offensivePart) => {',
  '    const li = document.createElement("li");',
  '    li.textContent = offensivePart.part + " " + "(" + offensivePart.offenses.join(", ") + ")";',
  '    ul.append(li);',
  '    analysisSummaryDetailsElement.append(ul);',
  '  });',
  '};',
  '',
  'const renderOffenses = (offenses) => {',
  '  const ul = document.createElement("ul");',
  '',
  '  offenses.forEach((offense) => {',
  '    const li = document.createElement("li");',
  '    li.textContent = offense;',
  '    ul.append(li);',
  '  });',
  '  analysisSummaryDetailsElement.insertBefore(ul, analysisSummaryDetailsElement.children[0]);',
  '};',
  '',
  'const renderAnalysisSummary = (analysisSummary, hasOffensiveParts) => {',
  '  analysisSummaryTextElement.textContent = analysisSummary;',
  '  if (!hasOffensiveParts) {',
  '    analysisSummaryDetailsElement.style.display = "none";',
  '    return;',
  '  }',
  '  analysisSummaryDetailsElement.style.display = "block";',
  '  analysisSummaryDetailsElement.innerHTML = "<p>The offending text parts are highlighted below:</p>";',
  '};',
  '',
  'const generateAnalysisInfo = (results) => {',
  '  let analysisSummary = "";',
  '  if (!results.some((result) => result.containsOffensiveAttributes === true)) {',
  '    analysisSummary = "You\'re all good to go! No toxic content detected.";',
  '    renderAnalysisSummary(analysisSummary, false);',
  '    return;',
  '  }',
  '  analysisSummary = "The paragraph was found to contain the following offensive attributes:";',
  '  let offensiveParts = [];',
  '  let offenses = [];',
  '  results.forEach((result) => {',
  '    let offensivePart = {',
  '      offenses: [],',
  '    };',
  '',
  '    if (result.containsOffensiveAttributes) {',
  '      offensivePart.part = result.sentence;',
  '      for (const [key, value] of Object.entries(result.outcomes)) {',
  '       if (value.value) {',
  '          offensivePart["offenses"].push(value.text);',
  '       }',
  '       if (value.value && offenses.indexOf(value.text) === -1) {',
  '          offenses.push(value.text);',
  '       }',
  '      }',
  '      offensiveParts.push(offensivePart);',
  '    }',
  '  });',
  '',
  '  renderAnalysisSummary(analysisSummary, true);',
  '  renderOffenses(offenses);',
  '  renderAnalysisDetails(offensiveParts);',
  '};',
  '',
  'const getPredictions = (sentences) => {',
  '  const results = sentences.map((sentence) => ({',
  '    sentence,',
  '    containsOffensiveAttributes: false,',
  '    outcomes: {',
  '      identity_attack: {',
  '        text: "Identity Attack",',
  '        value: false,',
  '      },',
  '      insult: {',
  '        text: "Insult",',
  '        value: false,',
  '      },',
  '      obscene: {',
  '        text: "Obscene",',
  '        value: false,',
  '      },',
  '      severe_toxicity: {',
  '        text: "Severely Toxicity",',
  '        value: false,',
  '      },',
  '      sexual_explicit: {',
  '        text: "Sexually Explicitness",',
  '        value: false,',
  '      },',
  '      threat: {',
  '        text: "Threat",',
  '        value: false',
  '      }',
  '    }',
  '  }));',
  '',
  '  toxicity.load(threshold).then((model) => {',
  '    model.classify(sentences).then((predictions) => {',
  '      predictions.forEach((prediction) => {',
  '        results.forEach((result, index) => {',
  '          if (result["outcomes"][prediction.label]) {',
  '            result["outcomes"][prediction.label]["value"] = prediction.results[index].match;',
  '          }',
  '          if (!result.containsOffensiveAttributes) {',
  '            result.containsOffensiveAttributes = prediction.results.some((res) => res.match === true);',
  '          }',
  '        });',
  '      });',
  '      generateAnalysisInfo(results);',
  '    });',
  '  });',
  '};',
  '',
  '',
  'toxicityModelFormElement.addEventListener("submit", (e) => {',
  '  e.preventDefault();',
  '',
  '  submitButtonElement.disabled = true;',
  '',
  '  const data = Object.fromEntries(new FormData(e.target).entries());',
  '  const paragraph = data.paragraph.trim();',
  '  if (paragraph === "") {',
  '    analysisElement.innerText = "Nothing to show. You have not entered anything!";',
  '    return;',
  '  }',
  '  const sentences = paragraph.split(/[.](?=.*[.])/g);',
  '',
  '  if (sentences.length <= 0) {',
  '    analysisElement.innerText = "Nothing to show. You have not entered some meaningful text!";',
  '    return;',
  '  }',
  '',
  '  getPredictions(sentences);',
  '  submitButtonElement.disabled = false;',
  '});',
  '',
].join('\n');
